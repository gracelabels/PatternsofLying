---
title: "Washington Post"
output:
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```

After the initial data cleaning described in our Data Cleaning document, there were 554 fact-checks published by the Washington Post between January 1st 2016 and June 30th, 2021. The dataset only includes fact-checks in which the claimant was a political figure, not Donald Trump, and assigned a Democratic or Republican affiliation. This dataset is called Version 8. We then did some light data cleaning prior to our analysis to clarify claimant names and streamline aberrant textual ratings. That process is described below.

```{r read v 8 wapo csv, warning=FALSE, message=FALSE, echo = FALSE}
v8_wapo <- read.csv("v8wapo.csv")
```

## Claimant Name Cleaning

In a program called OpenRefine, we created a new column called `claimant_original` to store original claimant names which are available for review. Then, we manually sorted the names of all claimants in our dataset in the `claimant` column to determine when there were different spellings of the same name. This was done via the cluster function embedded in the application. When varying iterations were found of the same name, we selected a single name to refer to each person by. For example Charles Schumer was changed to Chuck Schumer. For the `claimant` column, this cleaning process resulted in a reduction of claimants from 188 individual names to 169. This did not reduce the number of claims, just streamlined claimant names so the same political figure was not listed under several different names.

## Standardization of Textual Rating

Next we had to address the issues in `textualRating`. The Washington Post is not entirely consistent in its determinations of claim ratings. In theory, it has a standard ranking system ordered as follows: Geppetto Checkmark, One Pinocchio, Two Pinocchios, Three Pinocchios, Four Pinocchios. Of the 554 claims we pulled from the Washington Post, only 335 were originally ranked on the WaPo's five scale ratings system.

These definitions go as follows, sourced from <https://www.washingtonpost.com/politics/2019/01/07/about-fact-checker/>:

**The Geppetto Checkmark:** Statements and claims that contain "the truth, the whole truth, and nothing but the truth" will be recognized with our prized Geppetto checkmark. We tend to reserve this for claims that are unexpectedly true, so it is not awarded very often.

**One Pinocchio:** Some shading of facts. Selective telling of truth. Some omissions and exaggerations but no outright falsehoods. (You could view this as "mostly true.")

**Two Pinocchios:** Significant omissions and/or exaggerations. Some factual error may be involved but not necessarily. A politician can create a false, misleading impression by playing with words and using legalistic language that means little to ordinary people. (Similar to "half true.")

**Three Pinocchios:** Significant factual error and/or obvious contradictions. This gets into the realm of "mostly false." But it could include statements which are technically correct (such as based on official government data) but are so taken out of context as to be very misleading.

**Four Pinocchios:** Whoppers.

Our original data had these textual ratings:

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v8_wapo %>%
  group_by(textualRating) %>%
  count() %>%
  arrange(desc(n)) %>%
  kable()
```

Having the aim to analyze a body of claims all on the standardized rating scale, we began by reassigning certain ratings in OpenRefine based on the exact wording of the Washington Post's own definitions.

7 **mostly false** ratings were reassigned **Three Pinocchios**.\
4 **half true** ratings were reassigned to **Two Pinocchios**.\
7 **true**, **correct**, and **accurate** ratings were reassigned the **Geppetto Checkmark**

After this change, 353 claims were then rated on the standard ranking system.

201 claims remained tagged with anomaly ratings, meaning we could not reliably compare them to the rest of our data. To address this, we reached out to Glenn Kessler, the singular fact-checker at the Washington Post and brains behind the operation. He knows their scale inside and out. We sent over the remaining anomaly tags (sometimes grouping similar ones together) and received two instructions. First, he identified several tags that could be reassigned without further review. Second, he indicated several tags that he preferred to review and reassign himself. To not overburden Glenn during the manual review process, we removed all claims tagged with an anomaly rating that appeared only once or twice. For the first step, we turned to OpenRefine to make the changes he outlined. This was done to speed up the recoding process, though it could have been done in RStudio if necessary.

**Reassignments are the following:**\
Not the whole story ---\> Two Pinocchio\
Depends on math or on stats ---\> Two Pinocchio\
Wrong or incorrect ---\> Four Pinocchio\
Spins or twists the facts ---\> Three Pinocchio\
Exaggerated ---\> Three Pinocchio\
Cherry-picked number ---\> Two Pinocchio\
False ---\> Four Pinocchio

This reassigned 115 claims onto the standardized rating scale, leaving us with 468 claims on the 5-ranking scale.

Then for the claims that he preferred to self identify we gave him a list of all the claims tagged with the following ratings:\
Needs or lacks context\
Disputed or in dispute or study in dispute\
Misleading\
No evidence or lacks evidence

This amounted to 62 total ratings, 58 of which he assigned new ratings to. The remaining four were also removed from the data. This step was done in RStudio.

**In total, we were left with 526 claims of the original 554 now ranked by the standard Washington Post system, these are the ones we will use for continued analysis.**

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v9_wapo <- read.csv("v8wapo_adj_textRating_claimant.csv")
#this csv represents all changes made in OpenRefine, the earlier version of the dataset prior to these steps is above.

#reassigning glennclaims

v9_wapo$textualRating[v9_wapo$...1 == "14615"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14369"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13891"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14290"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13842"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13838"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13719"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14241"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13767"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14596"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "14418"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14285"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13484"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14105"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13781"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13878"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14630"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "3214"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13957"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14480"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14481"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14201"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14662"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13696"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13887"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14339"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13448"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14693"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13940"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "14131"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14534"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13540"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14116"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13894"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13420"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13403"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13370"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13371"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "13987"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13952"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14441"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13410"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13367"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13369"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14403"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14576"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13445"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13399"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13873"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14009"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14588"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14540"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14023"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14337"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14488"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13453"] <- "Three Pinocchios"

v9_wapo <- v9_wapo %>%
  filter(...1 != "13834", 
         ...1 != "14466", 
         ...1 != "14552", 
         ...1 != "14264")
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating <- v9_wapo %>%
  filter(textualRating == "One Pinocchio" |
           textualRating == "Two Pinocchios" |
           textualRating == "Three Pinocchios" |
           textualRating == "Four Pinocchios" |
           textualRating == "Geppetto Checkmark")

truenesscolors = c("#69B34C",
                   "#ACB334",
                   "#FDE64B",
                   "#FF872C",
                   "#FF0000")

v10_wapo_cleantextRating$textualRating <- 
  factor(v10_wapo_cleantextRating$textualRating, 
                              levels = c("Geppetto Checkmark", 
                                         "One Pinocchio", 
                                         "Two Pinocchios", 
                                         "Three Pinocchios", 
                                         "Four Pinocchios"))
```

## The Final Data

Utilizing this standardized dataset, the breakdown of claims by party and rating is found here.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  group_by(claimant_party) %>%
  count() %>%
  kable(caption = "Claim Counts by Party")

v10_wapo_cleantextRating %>%
  group_by(textualRating) %>%
  count() %>%
  kable(caption = "Claim Counts by Rating")

v10_wapo_cleantextRating %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(id_cols = claimant_party, 
              names_from = textualRating, 
              values_from = n,
              values_fill = 0) %>%
  kable(caption = "Claim Counts by Party and Rating")
```

Likely due to selection bias towards correcting falsehoods and/or an extremely high threshold of proof for truth, there are very few claims rated Geppetto Checkmark or One Pinocchio. As such, its hard to draw any significant conclusions from the limited data. The little information we have does appear to superficially indicate a higher ratio of Democratic truths to their Republican counterparts. There are dozens more datapoints under the Two, Three, and Four Pinocchios rating, both for Democratic and Republican politicians. By count, Democrat falsehoods exceed Republican's in the Two Pinocchios category, but they account for a lower proportion of claims rated Three and Four Pinocchios.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar() +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Counts by Party",
       x = "Rating",
       y = "Counts",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))  

v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Proportions by Party",
       x = "Rating",
       y = "Proportion",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))    
```

By proportion of total claims alone, and keeping in mind the limitations of any conclusion due to sample size, it appears that Democrats are fact-checked on more truthful claims, whereas Republicans account for a higher share of the egregious falsehoods checked by the Washington Post.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
ggplot(v10_wapo_cleantextRating, aes(x = claimant_party, fill = textualRating)) +
         geom_bar(position = "fill") +
  labs(title = "Proportion of Rating by Party",
       x = "Claims by Party",
       fill = "Rating", y = "Proportion") +
  scale_fill_manual(values = truenesscolors)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
dems <- v10_wapo_cleantextRating %>%
  filter(claimant_party == "Democratic")
dems <- dems %>%
  group_by(textualRating) %>%
  summarize(count = n()) %>%
  mutate(Democratic = 100*(count/nrow(dems))) %>%
  select(textualRating, Democratic)

reps <- v10_wapo_cleantextRating %>%
  filter(claimant_party == "Republican")
reps <- reps %>%
  group_by(textualRating) %>%
  summarize(count = n()) %>%
  mutate(Republican = 100*(count/nrow(reps))) %>%
  select(textualRating, Republican)

total <- v10_wapo_cleantextRating %>%
  group_by(textualRating) %>%
  summarize(count = n()) %>%
  mutate(Total = 100*(count/nrow(v10_wapo_cleantextRating))) %>%
  select(textualRating, Total)

textRatingcounts <- full_join(dems, reps, by = "textualRating") %>%
  full_join(total, by = "textualRating")

textRatingcounts %>%
  select(textualRating, Democratic, Republican, Total) %>%
  kable(digits = 1, 
        caption = "Percentage of Claims by Textual Rating", 
        format = "pipe")
```
