---
title: "Washington Post"
author: "Sofia Bliss-Carrascosa"
date: "10/05/2022"
output:
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```

After the initial data cleaning described in our Data Cleaning document, there were 554 fact-checks published by the Washington Post between January 1st 2016 and June 30th, 2021. The dataset only includes fact-checks in which the claimant was a political figure, not Donald Trump, and assigned a Democratic or Republican affiliation. This dataset is called Version 8. We then did some light data cleaning prior to our analysis to clarify claimant names and streamline aberrant textual ratings. That process is described below.

```{r read v 8 wapo csv}
v8_wapo <- read.csv("v8wapo.csv")
```

## Claimant Name Cleaning + Initial Rating Clean Up V9

In a program called OpenRefine, we created a new column called 'claimant_clean' and manually sorted the names of all claimants in our dataset to determine when there were different spellings of the same name. This was done via the cluster function embedded in the application. When varying iterations were found of the same name, we selected a single name to refer to each person by. For example Charles Schumer was changed to Chuck Schumer. For the claimant column, this cleaning process resulted in a reduction of claimants from 188 individual names to 169. This did not reduce the number of claims, just streamlined claimant names so the same political figure was not listed under several different names. The original claimant names are stored under the variable "claimant"

Next we had to address the issue in textualRating. The Washington Post is not entirely consistent in its determinations of claim ratings. In theory, it has a standard ranking system ordered as follows: Geppetto Checkmark, One Pinocchio, Two Pinocchios, Three Pinocchios, Four Pinocchios.

ADD FORMAL DEF

```{r}
v8_wapo %>%
  group_by(textualRating) %>%
  count() %>%
  arrange(desc(n)) %>%
  print(n=65)
```

However, in version 8 of our data, there were 335 are normal claims (those rated on the WaPo scale of 5 standardized ratings) and 219 anomaly claims with a non-standardized text based rating like, "Not the whole story" and "Lacks context."

Of those 219, 188 were labeled with an anomaly claim that was repeated more than once. To accord these common anomalies with the standard rating system, we took the data to Glenn Kessler, the singular WaPo fact-checker and the individual most familiar with the scale. At this stage we sent the more common ratings (occasionally grouping similar ones together) and asking him where they fall on the ratings scale.

Claims marked:

Wrong, Incorrect, False --\> Four Pinocchios Mostly false,

Spins or Twists facts, Exaggerated --\> Three Pinocchios

Half true, Not the whole story, Depends on the math, Cherry-picked number --\> Two Pinocchios

True, Correct, Accurate --\> Geppetto Checkmark

For the more unclear categories, we sent 61 claims for individual review. 57 of these were manually reassigned a textual rating according to his instructions, 4 were removed. For the purpose of concision, we removed claims that were tagged with uncommon anomaly ratings (1-2 observations). We began this process with 60 textual ratings, and reduced this down to the 5 rating ranking system.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v9_wapo <- read.csv("v8wapo-cleantextrating.csv")
#reassigning glennclaims

v9_wapo %>%
  group_by(textualRating) %>%
  summarise(count = n()) %>%
  print(n=40)

v9_wapo %>%
  distinct(claimant_clean)
v9_wapo$textualRating[v9_wapo$...1 == "14615"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14369"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13891"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14290"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13842"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13838"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13719"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14241"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13767"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14596"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "14418"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14285"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13484"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14105"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13781"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13878"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14630"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "3214"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13957"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14480"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14481"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14201"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14662"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13696"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13887"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14339"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13448"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14693"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13940"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "14131"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14534"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13540"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14116"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13894"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13420"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13403"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13370"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13371"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "13987"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13952"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14441"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13410"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13367"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13369"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14403"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14576"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13445"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13399"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13873"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14009"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14588"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14540"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14023"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14337"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14488"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13453"] <- "Three Pinocchios"

v9_wapo <- v9_wapo %>%
  filter(...1 != "13834", 
         ...1 != "14466", 
         ...1 != "14552", 
         ...1 != "14264")
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating <- v9_wapo %>%
  filter(textualRating == "One Pinocchio" |
           textualRating == "Two Pinocchios" |
           textualRating == "Three Pinocchios" |
           textualRating == "Four Pinocchios" |
           textualRating == "Geppetto Checkmark")

truenesscolors = c("#69B34C",
                   "#FDE64B",
                   "#FAB733",
                   "#FF872C",
                   "#FF0000")

v10_wapo_cleantextRating$textualRating <- 
  factor(v10_wapo_cleantextRating$textualRating, 
                              levels = c("Geppetto Checkmark", 
                                         "One Pinocchio", 
                                         "Two Pinocchios", 
                                         "Three Pinocchios", 
                                         "Four Pinocchios"))
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  group_by(claimant_party) %>%
  count() %>%
  kable(caption = "Claim Counts by Party")

v10_wapo_cleantextRating %>%
  group_by(textualRating) %>%
  count() %>%
  kable(caption = "Claim Counts by Rating")

v10_wapo_cleantextRating %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(id_cols = claimant_party, 
              names_from = textualRating, 
              values_from = n,
              values_fill = 0) %>%
  kable(caption = "Claim Counts by Party and Rating")
```

Likely due to selection bias towards correcting falsehoods and/or an extremely high threshold of proof for truth, there are very few claims rated Geppetto Checkmark or One Pinocchio. As such, its hard to draw any significant conclusions from the limited data. The little information we have does appear to superficially indicate a higher ratio of Democratic truths to their Republican counterparts. There are dozens more datapoints under the Two, Three, and Four Pinocchios rating, both for Democratic and Republican politicians. By count, Democrat falsehoods exceed Republican's in the Two Pinocchios category, but they account for a lower proportion of claims rated Three and Four Pinocchios.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Proportions by Party",
       x = "Rating",
       y = "Proportion",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))    

v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar() +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Counts by Party",
       x = "Rating",
       y = "Counts",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))  
```

By proportion of total claims alone, and keeping in mind the limitations of any conclusion due to sample size, it appears that Democrats are fact-checked on more truthful claims, whereas Republicans account for a higher share of the egregious falsehoods checked by the Washington Post.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
ggplot(v10_wapo_cleantextRating, aes(x = claimant_party, fill = textualRating)) +
         geom_bar(position = "fill") +
  labs(title = "Proportion of Rating by Party",
       x = "Claims by Party",
       fill = "Rating", y = "Proportion") +
  scale_fill_manual(values = truenesscolors)
```
