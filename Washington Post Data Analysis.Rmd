---
title: "Washington Post"
author: "Sofia Bliss-Carrascosa"
date: "10/05/2022"
output:
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```
We began with a dataset of Washington Post fact checks from Claim Review. We began with 554 observations and in the process of cleaning our data, described below, we ended with 530.

```{r}

```

In OpenRefine, we created a new column called ‘claimant_clean’ and manually sorted the names of all claimants to determine when there were different spellings of the same name. This was done via the cluster function embedded in the application. When iterations were found, we selected a single name to refer to each person by. For the claimant column, this cleaning process resulted in a reduction of claimants from 188 individual names to 169.

The Washington Post is not entirely consistent in its determinations of claim ratings. In theory, it has a standard ranking system ordered as follows: Geppetto Checkmark, One Pinocchio, Two Pinocchios, Three Pinocchios, Four Pinocchios. However, in our data, there were 335 are normal claims (those rated on the WaPo scale of 5 standardized ratings) and 173 anomaly claims with a non-standardized text based rating. Of those 219, 189 were labeled with an anomaly claim repeated more than once. To accord these common anomalies with the standard rating system, we took the data to Glenn Kessler, the singular WaPo fact-checker and the individual most familiar with the scale. At this stage we sent the more common ratings (occasionally grouping similar ones together) and asking him where they fall on the ratings scale. 
Claims marked:
Wrong, Incorrect, False --> Four Pinocchios
Mostly false, Spins or Twists facts, Exaggerated --> Three Pinocchios
Half true, Not the whole story, Depends on the math, Cherry-picked number --> Two Pinocchios
True, Correct, Accurate --> Geppetto Checkmark

For the more unclear categories, we sent 61 claims for individual review. 57 of these were manually reassigned a textual rating according to his instructions, 4 were removed. For the purpose of concision, we removed claims that were tagged with uncommon anomaly ratings (1-2 observations). We began this process with 60 textual ratings, having the goal of eventually reducing down to the 5 rating ranking system.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v9_wapo <- read.csv("v8wapo-cleantextrating.csv")
#reassigning glennclaims


v9_wapo$textualRating[v9_wapo$...1 == "14615"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14369"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13891"] <- "One Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14290"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13842"] <- "One Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13838"] <- "One Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13719"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14241"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13767"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14596"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "14418"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14285"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13484"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14105"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13781"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13878"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14630"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "3214"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13957"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14480"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14481"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14201"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14662"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13696"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13887"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14339"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13448"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14693"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13940"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "14131"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14534"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13540"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14116"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "13628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13894"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13420"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13403"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13370"] <- "Two Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13371"] <- "One Pinocchio"
v9_wapo$textualRating[v9_wapo$...1 == "13987"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13952"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14441"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13410"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13367"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13369"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14403"] <- "One Pinocchio" 
v9_wapo$textualRating[v9_wapo$...1 == "14576"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13445"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13399"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "13873"] <- "Four Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14009"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14628"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14588"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14540"] <- "Three Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14023"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14337"] <- "Two Pinocchios" 
v9_wapo$textualRating[v9_wapo$...1 == "14488"] <- "Three Pinocchios"
v9_wapo$textualRating[v9_wapo$...1 == "13453"] <- "Three Pinocchios"

v9_wapo <- v9_wapo %>%
  filter(...1 != "13834", 
         ...1 != "14466", 
         ...1 != "14552", 
         ...1 != "14264")
```


```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating <- v9_wapo %>%
  filter(textualRating == "One Pinocchio" |
           textualRating == "Two Pinocchios" |
           textualRating == "Three Pinocchios" |
           textualRating == "Four Pinocchios" |
           textualRating == "Geppetto Checkmark")

truenesscolors = c("#69B34C",
                   "#FDE64B",
                   "#FAB733",
                   "#FF872C",
                   "#FF0000")

v10_wapo_cleantextRating$textualRating <- 
  factor(v10_wapo_cleantextRating$textualRating, 
                              levels = c("Geppetto Checkmark", 
                                         "One Pinocchio", 
                                         "Two Pinocchios", 
                                         "Three Pinocchios", 
                                         "Four Pinocchios"))
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  group_by(claimant_party) %>%
  count() %>%
  kable(caption = "Claim Counts by Party")

v10_wapo_cleantextRating %>%
  group_by(textualRating) %>%
  count() %>%
  kable(caption = "Claim Counts by Rating")

v10_wapo_cleantextRating %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(id_cols = claimant_party, 
              names_from = textualRating, 
              values_from = n,
              values_fill = 0) %>%
  kable(caption = "Claim Counts by Party and Rating")
```

Likely due to selection bias towards correcting falsehoods and/or an extremely high threshold of proof for truth, there are very few claims rated Geppetto Checkmark or One Pinocchio. As such, its hard to draw any significant conclusions from the limited data. The little information we have does appear to superficially indicate a higher ratio of Democratic truths to their Republican counterparts. There are dozens more datapoints under the Two, Three, and Four Pinocchios rating, both for Democratic and Republican politicians. By count, Democrat falsehoods exceed Republican's in the Two Pinocchios category, but they account for a lower proportion of claims rated Three and Four Pinocchios.

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Proportions by Party",
       x = "Rating",
       y = "Proportion",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))    

v10_wapo_cleantextRating %>%
  ggplot(aes(x = textualRating, fill = claimant_party)) +
  geom_bar() +
  scale_fill_manual(values = c("blue2", "red3")) +
  labs(title = "Rating Counts by Party",
       x = "Rating",
       y = "Counts",
       fill = "Party of Claimant") + 
  theme(axis.text.x = element_text(size = 7))  
```

By proportion of total claims alone, and keeping in mind the limitations of any conclusion due to sample size, it appears that Democrats are fact-checked on more truthful claims, whereas Republicans account for a higher share of the egregious falsehoods checked by the Washington Post. 

```{r, warning=FALSE, message=FALSE, echo = FALSE}
ggplot(v10_wapo_cleantextRating, aes(x = claimant_party, fill = textualRating)) +
         geom_bar(position = "fill") +
  labs(title = "Proportion of Rating by Party",
       x = "Claims by Party",
       fill = "Rating", y = "Proportion") +
  scale_fill_manual(values = truenesscolors)
```

