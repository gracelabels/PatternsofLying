---
title: "PolitiFact Clean Up"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
library(tibble)
library(kableExtra)
```

```{r, warning=FALSE, message=FALSE, echo = FALSE}
v8_pf <- read_csv("v8pf.csv")
```

### Version 9 - Removal of PolitiFact anomalous claims

At this stage in the process, we had narrowed out data down to four publisher datasets from which we had sorted out all claims that did not fit our criteria of claims by U.S politician. We had also selected to eliminate Independent and Libertarian claims, as well as all claims by Donald Trump. Now within each publisher, we pivoted to focus on `textualRating`, which denotes the rating that the claim was assigned by the fact-checker. Some publishers have standardized scales with a ranking system, others do not. For each publisher we defined the existing rating scale, identified how many claims fit into that scale, and how many anomaly claims remained. First, we began working with PolitiFact.

PolitiFact has an established six tier rating scale called the Truth-O-Meter used to categorize claims.

As stated on PolitiFact's website:

"The goal of the Truth-O-Meter is to reflect the relative accuracy of a statement. The meter has six ratings, in decreasing level of truthfulness:

**TRUE** -- The statement is accurate and there's nothing significant missing.

**MOSTLY TRUE** -- The statement is accurate but needs clarification or additional information.

**HALF TRUE** -- The statement is partially accurate but leaves out important details or takes things out of context.

**MOSTLY FALSE** -- The statement contains an element of truth but ignores critical facts that would give a different impression.

**FALSE** -- The statement is not accurate.

**PANTS ON FIRE** -- The statement is not accurate and makes a ridiculous claim.

The burden of proof is on the speaker, and they rate statements based on the information known at the time the statement is made."

Below is a breakdown of all the ratings in the PolitiFact dataset's textualRating column.

There are: 2,527 standardized claims (those rated with one of the six standardized ratings) and 354 anomalous claims (those with a non-standardized text based rating).

Below the breakdown of the 20 most numerous ratings. It is not a complete list, but we place it below to show some examples.

```{r orig text ratings, warning=FALSE, message=FALSE, echo = FALSE}
v8_pf %>%
  group_by(textualRating) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(20) %>%
  kable(format = "pipe", caption = "V8, Sorted by Textual Rating")
```

When we reviewed the list of anomaly claims, we noticed that they all came from PolitiFact articles, not fact checks. PolitiFact only uses Truth-O-Meter ratings when they feel they can convey some level of certainty in the rating given. Truth-O-Meter ratings require a high threshold of proof. When that is lacking or there is not sufficient evidence to do a full scale fact-check, the existing facts are published in an article. Fact-checks of debates and speeches are frequently written up in articles. Since these claims represented instances where PolitiFact did not feel comfortable enough to deliver a standard Truth-O-Meter rating, we were unwilling to attribute a different level of certainty to the rating. Thus all claims that were not given one the 6 standard ratings in `textualRating` were removed from the dataset. This resulted in a remaining dataset of 2,527 claims.

```{r use pF ratings, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf <- v8_pf %>%
  filter(textualRating == "TRUE" |
           textualRating == "Mostly True" |
           textualRating == "Half True" |
           textualRating == "Mostly False" |
           textualRating == "FALSE" |
           textualRating == "Pants on Fire" )

v9_pf <- v9_pf %>%
  mutate(textualRating = case_when(textualRating == "TRUE" ~ "True",
                   textualRating == "FALSE" ~ "False",
                   textualRating == "Mostly True" ~ "Mostly True",
                   textualRating == "Half True" ~ "Half True",
                   textualRating == "Mostly False" ~ "Mostly False",
                   textualRating == "Pants on Fire" ~ "Pants on Fire"))

v9_pf$textualRating <- factor(v9_pf$textualRating, 
                              levels = c("Pants on Fire", 
                                         "False", 
                                         "Mostly False", 
                                         "Half True", 
                                         "Mostly True", 
                                         "True"))

v9_pf %>%
  group_by(textualRating) %>%
  count() %>%
  kable(format = "pipe", caption = "V9, Truth-O-Meter Rating Distribution")
```

### Version 10 - Condensing claimant names

After this removal we began looking closely at the remaining claimant names. Version 9 had 724 unique claimant names, below are the first 20.

```{r pf people, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf %>%
  group_by(claimant) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(20) %>%
  kable(format = "pipe", caption = "V9, Claimant Names by Occurences")
```

We noticed that some claimants were listed under several separate names despite referring to the same person, like Speaker Nancy Pelosi, Nancy Pelosi, Speaker Pelosi. We wanted to eliminate this redundancy so that we could see the true number of claims made by each claimant. To do so, we moved our data into a program called OpenRefine. Here, we clustered claimant names to identify where multiple names were used to refer to the same person. We used this to identify all duplicate forms of claimant names and then manually recoded the data accordingly in RStudio.

This processed combined several names making the list 27 names shorter. 697 unique claimants resulted. The 20 most numerous claimants are listed below.

```{r condense claimant names, warning=FALSE, message=FALSE, echo = FALSE}
v10_pf <- v9_pf

v10_pf$claimant[v10_pf$claimant == "Andrew M. Cuomo"] <- "Andrew Cuomo"
v10_pf$claimant[v10_pf$claimant == "U.S. Rep. Ann Kirkpatrick"] <- "Ann Kirkpatrick"
v10_pf$claimant[v10_pf$claimant == "Charles Schumer"] <- "Chuck Schumer"
v10_pf$claimant[v10_pf$claimant == "U.S. Sen. Charles Schumer"] <- "Chuck Schumer"
v10_pf$claimant[v10_pf$claimant == "Edward J. Markey"] <- "Ed Markey"
v10_pf$claimant[v10_pf$claimant == "Edward Markey"] <- "Ed Markey"
v10_pf$claimant[v10_pf$claimant == "Gordon Hintz; D-Oshkosh;"] <- "Gordon Hintz"
v10_pf$claimant[v10_pf$claimant == "Gov. Rick Scott"] <- "Rick Scott"
v10_pf$claimant[v10_pf$claimant == "Sen. Rick Scott"] <- "Rick Scott"
v10_pf$claimant[v10_pf$claimant == "Greg Abbot"] <- "Greg Abbott"
v10_pf$claimant[v10_pf$claimant == "J.B. Pritzker"] <- "J. B. Pritzker"
v10_pf$claimant[v10_pf$claimant == "JB Pritzker"] <- "J. B. Pritzker"
v10_pf$claimant[v10_pf$claimant == "Janel Brandtjen;"] <- "Janel Brandtjen"
v10_pf$claimant[v10_pf$claimant == "Julián Castro"] <- "Julian Castro"
v10_pf$claimant[v10_pf$claimant == "Rev. Mark Harris"] <- "Mark Harris"
v10_pf$claimant[v10_pf$claimant == "Mark Pocan;"] <- "Mark Pocan"
v10_pf$claimant[v10_pf$claimant == "Mike Bloomberg"] <- "Michael Bloomberg"
v10_pf$claimant[v10_pf$claimant == "Michael Kearns"] <- "Michael P. Kearns"
v10_pf$claimant[v10_pf$claimant == "Nancy Pelosi; D-Calif."] <- "Nancy Pelosi"
v10_pf$claimant[v10_pf$claimant == "Nicholas Langworthy"] <- "Nick Langworthy"
v10_pf$claimant[v10_pf$claimant == "Rep. Tom Reed"] <- "Tom Reed"
v10_pf$claimant[v10_pf$claimant == "Sen. Roy Blunt"] <- "Roy Blunt"
v10_pf$claimant[v10_pf$claimant == "Sen. Sherrod Brown"] <- "Sherrod Brown"
v10_pf$claimant[v10_pf$claimant == "Terry McAuliffe; D"] <- "Terry McAuliffe"
v10_pf$claimant[v10_pf$claimant == "Thomas Perez"] <- "Tom Perez"
v10_pf$claimant[v10_pf$claimant == "Rep. Chris Collins"] <- "Chris Collins"
v10_pf$claimant[v10_pf$claimant == "Beto O’Rourke"] <- "Beto O'Rourke"
v10_pf$claimant[v10_pf$claimant == "Thom TIllis"] <- "Thom Tillis" 
v10_pf$claimant[v10_pf$claimant == "Thom TIllis"] <- "Thom Tillis" 
v10_pf$claimant[v10_pf$claimant == "David R. Lewis"] <- "David Lewis"
```

```{r v10 table, warning=FALSE, message=FALSE, echo = FALSE}
v10_pf %>%
  group_by(claimant) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(20) %>%
  kable(format = "pipe", caption = "V10, Claimant Names without Duplicates")
```

### Version 11 -- Final Claimant Cleaning

During this process we also noticed that some claimants in the data who did not fit our defintion of politician, yet had not been identified as such by our code. To ensure that the claimants fit the specified definition of politician, we ran the list of claimant names through a stricter version of the politician filter. 58 names were marked as potentially not being politicial figures. Each name was manually reviewed and we identified 6 names that did not belong in the dataset: Tucker Carlson, Laura Ingraham, Jacob Wohl, State representatives, Reagan was Right, and Marco Rubio's heckler.

21 claims were removed as a result.

Later on during tagging we identified three more claimants that were not political figures (Pat Robertson, Juan Williams, and Evan Smith) and 1 claim that was mislabeled (our data said it was spoken by Maxine Waters but the PolitiFact article listed the claimant as "bloggers").

For ease we have addressed them at this stage, thus 6 additional claims were removed.

```{r create v11, warning=FALSE, message=FALSE, echo = FALSE}
v11_pf <- v10_pf %>%
  filter(claimant != "Tucker Carlson",
           claimant != "Laura Ingraham",
           claimant != "Jacob Wohl",
           claimant != "State representatives",
           claimant != "Reagan was Right",
           claimant != "Marco Rubio's heckler",
           claimant != "Pat Robertson",
           claimant != "Evan Smith",
           claimant != "Juan Williams") %>%
  filter(...1 != 3120) #Maxine Waters claim 
```

Below are the counts for the final dataset used for tagging.

```{r v11table, warning=FALSE, message=FALSE, echo = FALSE}
v11_pf %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(names_from = claimant_party, id_cols = textualRating, values_from = n) %>%
  kable(format = "pipe", caption = "V11, Ratings by Party")
```

```{r date error fixed,  warning=FALSE, message=FALSE, echo = FALSE}
v11_pf$claimDate[v11_pf$claimDate == (as.Date("2106-11-08"))] <- as.Date("2016-11-08")
#During this process an erroneous claim came to our attention, dated 2106 instead of 2016. We manually recoded this.
```

### Tagging Claims

Now that we had a workable dataset, we wanted to learn more about what politicians were lying about. We subject tagged each claim in the data set with relevant and comprehensive tags to categorize the topics of lies and identify patterns in lying.\
\
We decided to only tag False(ish) claims (Mostly False, False, and Pants on Fire) because our aim was to study what politicians are lying about, not what they are telling the truth about. We acknowledge that this choice limits our inquiry, but since claims were going to be tagged manually we decided to focus on our primary research question. This left 1,085 False(ish) claims to be subject tagged.

```{r count of falseish pf, message=FALSE, warning=FALSE, echo=FALSE}
v11_pf %>%
  filter(textualRating == "False" |
           textualRating == "Mostly False" | 
           textualRating == "Pants on Fire") %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(names_from = claimant_party, id_cols = textualRating, values_from = n) %>%
  kable(format = "pipe", caption = "False(ish) Claims to be Tagged")
```

We then had to construct a method for tagging claims. To not start from scratch, we decided to ground our project in an established tagging framework drawing from Frank Baumagrtner's well established subject tags used for the [Comparative Agendas Project](https://www.comparativeagendas.net/pages/master-codebook).\
\
However, since these were designed to tag policy platforms and not political speech, there were some categories missing from Baumgartner's tags that were common among the claims we analyzed. To address this, our research team conducted a series of practice tagging sessions in which 100 randomly selected claims were subject tagged. Through this process we discovered which categories we felt were relevant but missing, allowing us to establish categories that best encompassed the types of claims found in the dataset. The tagging system and tags themselves were built based on the data we were tagging.\
\
We defined tags by supplying a list of topics that fell underneath that subject area. We identified these with buzzwords that could be seen in a claim, or general concepts that could help a tagger tag the claims correctly. Despite every effort to be comprehensive, we recognize there may be some remaining gaps in our definitions.\
\
Words in parentheses signify modifications/clarifications to the definition after tagging had begun.\
\
People may disagree with our category grouping. We acknowledge the subjective nature of these categories.

Micro tags were traditional subject tags that describe the substance of the claim and the political issues or policy issues discussed in it.\
Macro tags take a more big picture approach describing the directive of the claim (Opponent/Self/Legislation) and the use of Fear within a claim.

```{r loading subject tag table, message=FALSE, warning=FALSE, echo=FALSE}
subjecttags <- read_csv("SubjectTagDefinitions.csv")

```

```{r micro suject table, message=FALSE, warning=FALSE, echo=FALSE }
subjecttags %>%
  slice(1:32) %>%
  kable(caption = "Micro Tag Definitions", digits = 2) %>%
  kable_styling(font_size = 6, full_width = TRUE)
```

```{r macro subject table, message=FALSE, warning=FALSE, echo=FALSE }
subjecttags %>%
  slice(33:37) %>%
  kable(caption = "Macro Tag Definitions", digits = 2) %>%
  kable_styling(font_size = 6, full_width = TRUE)

```

\pagebreak

Through tagging, we wanted to be able to describe as many relevant attributes of a claim as possible. First, we allowed a claim to be assigned multiple tags, and we had different types of tags-- Macro and Micro.

Four coders were trained on how to tag claims based on the subject tag definitions. Taggers did practice rounds on randomly selected claims and we talked through instances where they disagreed and worked to standardize our definitions.

We were actively seeking consensus between our taggers. However, because each claim could receive multiple tags, one different tag would mean a mismatch and high inter-coder reliability would be hard to achieve.\
\
Asa Royal, a software engineer in the Duke Reporters Lab, created an online tagging interface for our coders to use. A coder was presented with only the text of a claim, and a link the fact-check if they needed more information. They were not shown the claimant's name or party affiliation unless they reviewed the URL. They would then select all the tags they felt applied to that claim. All claims were tagged by at least two coders. If the two coders agreed on the tags, that claim's tag were not reviewed. But, if there was disagreement between the two coders, as was the case for 776 of the claims, lead researcher Grace Abels manually reviewed each claim and the two different sets of tags that had been assigned. Grace Abels then determined the final tags for each claim.

In retrospect, we found two of our categories to be ineffective in practice: the Fear macro tag and the Record/Candidate Biography/Campaigns and Personal Behavior micro tag. Though we were comfortable with the theoretical definition we came up with, we found tagging for Fear to be extremely difficult, resulting in significant disagreements between taggers. We were often confronted with our own biases, struggling to identify what was inherently frightening, and what we interpreted as fear-driven based on our political, social, or cultural views. A common sentiment among taggers was: *I do not personally find this frightening, but could see how if I held a certain belief/political perspective it would be.* While we decided to leave this tag available for review, we want to acknowledge it's extremely subjective nature and inform readers that the application of the Fear tag may be inconsistent. The issue in the Record/Candidate Biography/Campaigns and Personal Behavior micro tag originated from its definition, which we found to be problematic as we utilized it in practice. It was specified to refer to claims where there speaker referenced a single individual, however, in practice we found that definition to be too narrow. We recommend the use of the Self/Personal Record and Opponent/Opponent's Record tags because we feel they are more descriptive of the content of the claim.

Understanding the confounding factors that come with our method of tagging, we feel confident in our system. The tagged data is available for review. After tagging, the dataset was joined with the larger version 11.

```{r createmetadata, warning=FALSE, message=FALSE, echo = FALSE}
taggedclaims <- read_csv("tagged_claims_limited.csv")

pf_mega_withdupes <- left_join(v11_pf, taggedclaims, by = c("...1", 
                                                            "url", 
                                                            "languageCode", 
                                                            "publisher.name", 
                                                            "publisher.site", 
                                                            "reviewDate", 
                                                            "text", 
                                                            "claimDate", 
                                                            "claimant_party"))
```

```{r rename columns, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_withdupes <- pf_mega_withdupes %>%
  select(-title.y, -textualRating.y, -claimant.y)
pf_mega_withdupes$title <- pf_mega_withdupes$title.x
pf_mega_withdupes$textualRating <- pf_mega_withdupes$textualRating.x
pf_mega_withdupes$claimant <- pf_mega_withdupes$claimant.x
pf_mega_withdupes <- pf_mega_withdupes %>%
  select(-title.x, -textualRating.x, -claimant.x)
```

### Manual Removal of Duplicate Claims

As mentioned earlier, 31 more claims were identified as duplicates during tagging. These were claims with differing urls but identical texts. These were not captured earlier because we searched for duplicates with identical urls. Most often these duplicates were fact-checks that had already been published and were included again in an article round-up of fact-checks.

```{r finddupes, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_withdupes %>%
  group_by(text) %>%
  count() %>%
  filter(n > 1) %>%
  arrange(desc(n)) %>%
  kable(format = "pipe", caption = "Remaining Duplicates Identified during Tagging")
```

We prioritized preserving the claims linked to the original and specific fact check over those republished as part of a larger article. To do so, we manually removed the 31 duplicates by their claim identifier number. This resulted in a remaining 2469 claims.

```{r finaldedupe, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_nodupes <- pf_mega_withdupes %>%
  filter(...1 != 3254, 
         ...1 != 7377,
         ...1 != 7378,
         ...1 != 4553,
         ...1 != 4552,
         ...1 != 5740,
         ...1 != 7580,
         ...1 != 7581,
         ...1 != 7229,
         ...1 != 4176,
         ...1 != 4177,
         ...1 != 3425,
         ...1 != 1927,
         ...1 != 2627,
         ...1 != 4737,
         ...1 != 4738,
         ...1 != 7622,
         ...1 != 6597,
         ...1 != 7291,
         ...1 != 7292,
         ...1 != 460,
         ...1 != 3991,
         ...1 != 7088,
         ...1 != 7089,
         ...1 != 3313,
         ...1 != 6408,
         ...1 != 6409,
         ...1 != 7402,
         ...1 != 3383,
         ...1 != 3382,
         ...1 != 3381)
```

### Location of Lie

In addition to the Micro and Macro tags, we also tagged the entire dataset, not just False(ish) claims, by the location of the lie, meaning the medium and format of the statement containing the lie. This tracked attributes such as whether it was said on TV, in an interview, on social media, etc. We had several overarching categories called `location` tags, and then more specific subcategories tagged `location.extra`.

```{r locoflie, warning=FALSE, message=FALSE, echo = FALSE}
loc_of_lie <- read.csv("locationoflie.csv")
#When loading this data into R, we made a select number of changes to make it compatible with the existing data. We selected to join the location of lie data to the larger dataset by the claim identifier column, ...1, and by url, languageCode, publisher.name, publisher.site, text, claimDate, and claimant_party. We chose not to merge by title or text of claim due to a small number of claims that were encoded oddly with incorrect symbols. This error likely occured during exportation and importation of the dataset. The correct title and text were pulled from the megadata, and we are certain these are still joined correctly thanks to the claim identifier column. We found one more incorrect claim that was missing the correct text, we sourced the accurate claim from the URL and overwrote it in our data. There was one claim that was erroneously untagged with location information, this was fixed. 
loc_of_lie <- loc_of_lie %>%
  select(-claimant,
         -textualRating, 
         -reviewDate)
loc_of_lie <- loc_of_lie %>%
  mutate(claimDate = as.Date(claimDate))
loc_of_lie$claimDate[loc_of_lie$claimDate == 
                       (as.Date("2106-11-08"))] <- as.Date("2016-11-08")
#We matched the name of the claim identifier column in the location of lie data to be identical to the mega data and selected only half the variables leaving only new information and the variables needed to join the two datasets correctly. We also found that one of the variables had not loaded in correctly, so we informed R that the claimDate column was, in fact, containing time/date data and fixed one erroneous claim where the year was misstyped 2106 instead of 2016.
```

Finally, we exported the final dataset into a viewable and downloadable csv, called `pf_mega_location.csv`.

Further analysis of this data can be found in the Location of Lie Analysis, available in both accessible RMD and PDF form.

```{r megalocation, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_location <- left_join(pf_mega_nodupes, 
                              loc_of_lie, 
                              by = c("...1", 
                                     "url", 
                                     "languageCode", 
                                     "publisher.name", 
                                     "publisher.site", 
                                     "text", 
                                     "claimDate", 
                                     "claimant_party"))
pf_mega_location$title <- pf_mega_location$title.x
pf_mega_location <- pf_mega_location %>%
  select(-title.x, -title.y)

pf_mega_location$text[pf_mega_location$...1 == 2766] <- 
  "The Trans-Pacific trade deal could cost America 448,000 more jobs."

pf_mega_location$location[pf_mega_location$...1 == 7539] <- 
  "Social Media"
pf_mega_location$location.extra[pf_mega_location$...1 == 7539] <- 
  "Twitter"

write_csv(pf_mega_location, file = "pf_mega_location.csv")
```

We continue to analyze this data in the PolitiFact Analysis document.
