---
title: "PolitiFact Clean Up"
author: "Grace Abels"
date: "4/19/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```

```{r}
v8_pf <- read_csv("v8pf.csv")
```

### Version 8 - Removal of PolitiFact anomalous claims

"At this part of the process, we have sorted out all claims that do not fit our criteria of claims by U.S politician from out 4 publisher datasets. We have also eliminated Independent and Libertarian claims. Now within each publisher we zoom into the field textualRating. This denotes the rating that the claim was given by the fact-checker. Some have standardized scales with ranking others do not. For this step we have defined the existing rating scale, seen how many fit into that scale, and how many anomaly claims remain.

For PolitiFact, they have an established 6 tier rating scale called the Truth-O-Meter in which claims are assigned one of the following ratings. The scale goes from more true to less true. This information was pulled from PolitiFact's website

"The goal of the Truth-O-Meter is to reflect the relative accuracy of a statement. The meter has six ratings, in decreasing level of truthfulness:"

**TRUE** -- The statement is accurate and there's nothing significant missing.

**MOSTLY TRUE** -- The statement is accurate but needs clarification or additional information.

**HALF TRUE** -- The statement is partially accurate but leaves out important details or takes things out of context.

**MOSTLY FALSE** -- The statement contains an element of truth but ignores critical facts that would give a different impression.

**FALSE** -- The statement is not accurate.

**PANTS ON FIRE** -- The statement is not accurate and makes a ridiculous claim.

The burden of proof is on the speaker, and we rate statements based on the information known at the time the statement is made.

Below is a breakdown of all the ratings in PF textualRating. There are

2,527 Normal claims (those rated with on of the 6 standardized ratings)

354 Anomalous claims (those with a non-standardized text based rating)

```{r orig text ratings, warning=FALSE, message=FALSE, echo = FALSE}
v8_pf %>%
  group_by(textualRating) %>%
  count() %>%
  arrange(desc(n)) %>%
  kable(caption = "Textual Ratings")
```

When we reviewed the list of anomaly claims, we noticed that all came from PolitiFact articles. PolitiFact values their Truth-O-Meter ratings and use them only when they feel they can convey some level of certainty in the rating given. Truth-O-Meter ratings require a high threshold of proof. When that is lacking or there is not evidence to do a full scale fact-check the existing facts are published in an article. Fact-checks of debates and speeches are frequently written up in articles. Since PF did not feel comfortable enough to deliver a full Truth-O-Meter rating neither did we and all claims that were not given on the 6 standard ratings in textualRating were removed from the dataset.

### Version 9 - Standardized Ratings Only

```{r use pF ratings, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf <- v8_pf %>%
  filter(textualRating == "TRUE" |
           textualRating == "Mostly True" |
           textualRating == "Half True" |
           textualRating == "Mostly False" |
           textualRating == "FALSE" |
           textualRating == "Pants on Fire" )

v9_pf <- v9_pf %>%
  mutate(textualRating = case_when(textualRating == "TRUE" ~ "True",
                   textualRating == "FALSE" ~ "False",
                   textualRating == "Mostly True" ~ "Mostly True",
                   textualRating == "Half True" ~ "Half True",
                   textualRating == "Mostly False" ~ "Mostly False",
                   textualRating == "Pants on Fire" ~ "Pants on Fire"))

v9_pf$textualRating <- factor(v9_pf$textualRating, 
                              levels = c("Pants on Fire", 
                                         "False", 
                                         "Mostly False", 
                                         "Half True", 
                                         "Mostly True", 
                                         "True"))

v9_pf %>%
  group_by(textualRating) %>%
  count() %>%
  kable(caption = "PF Claims with Truth-O-Meter Ratings")
```

After this removal we began looking closely at the remaining claimant names. Version 8 had 723 unique claimant names.

```{r pf people, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf %>%
  group_by(claimant) %>%
  count() %>%
  arrange(desc(n))
```

### Version 10 - Condensing claimant names

At this stage we noticed that some claimants were listed under several separate names referring to the same person. Like Speaker Nancy Pelosi, Nancy Pelosi, Speaker Pelosi. We wanted to eliminate this repetition so that we could see the true \# of claims made by each claimant.

This processed combined several names making the list 25 names shorter.

698 unique claimants remain. WRITE ABOUT OPEN REFINE CLAIMANT CLEANING

```{r condense claimant names, warning=FALSE, message=FALSE, echo = FALSE}
v10_pf <- v9_pf

v10_pf$claimant[v10_pf$claimant == "Andrew M. Cuomo"] <- "Andrew Cuomo"
v10_pf$claimant[v10_pf$claimant == "U.S. Rep. Ann Kirkpatrick"] <- "Ann Kirkpatrick"
v10_pf$claimant[v10_pf$claimant == "Charles Schumer"] <- "Chuck Schumer"
v10_pf$claimant[v10_pf$claimant == "U.S. Sen. Charles Schumer"] <- "Chuck Schumer"
v10_pf$claimant[v10_pf$claimant == "Edward J. Markey"] <- "Ed Markey"
v10_pf$claimant[v10_pf$claimant == "Edward Markey"] <- "Ed Markey"
v10_pf$claimant[v10_pf$claimant == "Gordon Hintz; D-Oshkosh;"] <- "Gordon Hintz"
v10_pf$claimant[v10_pf$claimant == "Gov. Rick Scott"] <- "Rick Scott"
v10_pf$claimant[v10_pf$claimant == "Sen. Rick Scott"] <- "Rick Scott"
v10_pf$claimant[v10_pf$claimant == "Greg Abbot"] <- "Greg Abbott"
v10_pf$claimant[v10_pf$claimant == "J.B. Pritzker"] <- "J. B. Pritzker"
v10_pf$claimant[v10_pf$claimant == "JB Pritzker"] <- "J. B. Pritzker"
v10_pf$claimant[v10_pf$claimant == "Janel Brandtjen;"] <- "Janel Brandtjen"
v10_pf$claimant[v10_pf$claimant == "Julián Castro"] <- "Julian Castro"
v10_pf$claimant[v10_pf$claimant == "Rev. Mark Harris"] <- "Mark Harris"
v10_pf$claimant[v10_pf$claimant == "Mark Pocan;"] <- "Mark Pocan"
v10_pf$claimant[v10_pf$claimant == "Mike Bloomberg"] <- "Michael Bloomberg"
v10_pf$claimant[v10_pf$claimant == "Michael Kearns"] <- "Michael P. Kearns"
v10_pf$claimant[v10_pf$claimant == "Nancy Pelosi; D-Calif."] <- "Nancy Pelosi"
v10_pf$claimant[v10_pf$claimant == "Nicholas Langworthy"] <- "Nick Langworthy"
v10_pf$claimant[v10_pf$claimant == "Rep. Tom Reed"] <- "Tom Reed"
v10_pf$claimant[v10_pf$claimant == "Sen. Roy Blunt"] <- "Roy Blunt"
v10_pf$claimant[v10_pf$claimant == "Sen. Sherrod Brown"] <- "Sherrod Brown"
v10_pf$claimant[v10_pf$claimant == "Terry McAuliffe; D"] <- "Terry McAuliffe"
v10_pf$claimant[v10_pf$claimant == "Thomas Perez"] <- "Tom Perez"
v10_pf$claimant[v10_pf$claimant == "Rep. Chris Collins"] <- "Chris Collins"
v10_pf$claimant[v10_pf$claimant == "Beto O’Rourke"] <- "Beto O'Rourke"
v10_pf$claimant[v10_pf$claimant == "Thom TIllis"] <- "Thom Tillis" 
v10_pf$claimant[v10_pf$claimant == "Thom TIllis"] <- "Thom Tillis" 
v10_pf$claimant[v10_pf$claimant == "David R. Lewis"] <- "David Lewis"
```

```{r v10 table, warning=FALSE, message=FALSE, echo = FALSE}
v10_pf %>%
  group_by(claimant) %>%
  count() %>%
  arrange(desc(n))
```

### Version 11 -- Final Claimant Cleaning

During this process we also noticed that some claimants, who did not fit our defintion of politician, had slipped through the cracks in our code. To try and ensure that we had only the data we desired in our dataset, we ran the list of claimant names through a stricter version of the politician filter. 58 names were marked as potentially non-politicial figures. Each name was manually reviewed and we identified 6 names that did not belong in the dataset.

Tucker Carlson, Laura Ingraham, Jacob Wohl, State representatives, Reagan was Right, Marco Rubio's heckler. 21 claims were removed as a result.

Later on during tagging we identified three more claimants (Pat Robertson, Juan Williams, and Evan Smith) and 1 claim that was mislabeled (The claim said it was Maxine Waters but the link said it was bloggers) that were not political figures. For ease we have removed them here. 6 more claims were removed.

```{r create v11, warning=FALSE, message=FALSE, echo = FALSE}
v11_pf <- v10_pf %>%
  filter(claimant != "Tucker Carlson",
           claimant != "Laura Ingraham",
           claimant != "Jacob Wohl",
           claimant != "State representatives",
           claimant != "Reagan was Right",
           claimant != "Marco Rubio's heckler",
           claimant != "Pat Robertson",
           claimant != "Evan Smith",
           claimant != "Juan Williams") %>%
  filter(...1 != 3120) #Maxine Waters claim 
```

Below are the counts for the final dataset used for tagging.

```{r v11table, warning=FALSE, message=FALSE, echo = FALSE}
v11_pf %>%
  group_by(claimant_party, textualRating) %>%
  count() %>%
  pivot_wider(names_from = claimant_party, id_cols = textualRating, values_from = n) %>%
  kable(caption = "Claim Rating by Party for Final Dataset")
```

EXPLAIN DATE ERROR ANF MOVE UP

```{r date error fixed,  warning=FALSE, message=FALSE, echo = FALSE}
v11_pf$claimDate[v11_pf$claimDate == (as.Date("2106-11-08"))] <- as.Date("2016-11-08")
```

### Tagged Claims

HOW WE TAGGED AND WHY

WHY WE JOINED BY JUST THESE VARIABLS

```{r createmetadata, warning=FALSE, message=FALSE, echo = FALSE}
taggedclaims <- read_csv("tagged_claims_limited.csv")

pf_mega_withdupes <- left_join(v11_pf, taggedclaims, by = c("...1", "url", "languageCode", "publisher.name",
"publisher.site", "reviewDate", "text", "claimDate", "claimant_party"))
```

```{r rename columns, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_withdupes <- pf_mega_withdupes %>%
  select(-title.y, -textualRating.y, -claimant.y)
pf_mega_withdupes$title <- pf_mega_withdupes$title.x
pf_mega_withdupes$textualRating <- pf_mega_withdupes$textualRating.x
pf_mega_withdupes$claimant <- pf_mega_withdupes$claimant.x
pf_mega_withdupes <- pf_mega_withdupes %>%
  select(-title.x, -textualRating.x, -claimant.x)
```

Manual Removal of Duplicate Claims\
\
EXPLAIN DUPLICATE REMOVAL

```{r finddupes, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_withdupes %>%
  group_by(text) %>%
  count() %>%
  filter(n > 1) %>%
  arrange(desc(n)) %>%
  kable(caption = "Remaining Duplicates Identified during Tagging")
```

```{r finaldedupe, warning=FALSE, message=FALSE, echo = FALSE}
pf_mega_nodupes <- pf_mega_withdupes %>%
  filter(...1 != 3254, 
         ...1 != 7377,
         ...1 != 7378,
         ...1 != 4553,
         ...1 != 4552,
         ...1 != 5740,
         ...1 != 7580,
         ...1 != 7581,
         ...1 != 7229,
         ...1 != 4176,
         ...1 != 4177,
         ...1 != 3425,
         ...1 != 1927,
         ...1 != 2627,
         ...1 != 4737,
         ...1 != 4738,
         ...1 != 7622,
         ...1 != 6597,
         ...1 != 7291,
         ...1 != 7292,
         ...1 != 460,
         ...1 != 3991,
         ...1 != 7088,
         ...1 != 7089,
         ...1 != 3313,
         ...1 != 6408,
         ...1 != 6409,
         ...1 != 7402,
         ...1 != 3383,
         ...1 != 3382,
         ...1 != 3381)
```

\

One of the things that we tagged by was location of the lie, meaning the medium and format of the statement containing the lie. This tracked things like whether it was said on TV, in an interview, on social media, etc. We had several metacategories called `location` tags, and then more specific subcategories tagged `location.extra`. When loading this data into R, we made a select number of changes to make it compatible with the existing data. We matched the claim identifier column in the location of lie data to be identical to the mega data, and selected only half the variables leaving only new information and the variables needed to join the two datasets correctly. We also found that one of the variables had not loaded in correctly, so we informed R that the `claimDate` column was, in fact, containing time/date data. We fixed one erroneous claim where the year was misstyped 2106 instead of 2016.

```{r locoflie, warning=FALSE, message=FALSE, echo = FALSE}
loc_of_lie <- read.csv("locationoflietagging.csv")
loc_of_lie$...1 <- loc_of_lie$X.1
loc_of_lie <- loc_of_lie %>%
  select(-claimant, -X.1, -claimant_canonical_name, -reason, -ratingsimp, -`T.F.RQ1`, -X, -rand_num, -is_politician_strict, -textualRating, -reviewDate)
loc_of_lie <- loc_of_lie %>%
  mutate(claimDate = as.Date(claimDate))
loc_of_lie$claimDate[loc_of_lie$claimDate == (as.Date("2106-11-08"))] <- as.Date("2016-11-08")
```

EXPLAIN JOIN\

```{r}
pf_mega_location <- left_join(pf_mega_nodupes, loc_of_lie, by = c("...1", "url", "languageCode", "publisher.name", "publisher.site", "text", "claimDate", "claimant_party"))
pf_mega_location$title <- pf_mega_location$title.x
pf_mega_location <- pf_mega_location %>%
  select(-title.x, -title.y)

pf_mega_location$text[pf_mega_location$...1 == 2766] <- "The Trans-Pacific trade deal could cost America 448,000 more jobs."

write_csv(pf_mega_location, file = "pf_mega_location.csv")
```
