---
title: "PolitiFact Analysis"
author: "Grace Abels"
date: "4/19/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r load packages, warning=FALSE, message=FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
```

```{r}
v8_pf <- read_csv("v8pf.csv")
```

"At this part of the process, we have sorted out all claims that do not fit our criteria of claims by U.S politician from out 4 publisher datasets. We have also eliminated Independent and Libertarian claims. Now within each publisher we zoom into the field textualRating. This denotes the rating that the claim was given by the fact-checker. Some have standardized scales with ranking others do not. For this step we have defined the existing rating scale, seen how many fit into that scale, and how many anomaly claims remain. 

For PolitiFact, they have an established 6 tier rating scale called the Truth-O-Meter in which claims are assigned one of the following ratings. The scale goes from more true to less true. This information was pulled from PolitiFact's website

"The goal of the Truth-O-Meter is to reflect the relative accuracy of a statement. The meter has six ratings, in decreasing level of truthfulness:"

TRUE – The statement is accurate and there’s nothing significant missing.

MOSTLY TRUE – The statement is accurate but needs clarification or additional information.

HALF TRUE – The statement is partially accurate but leaves out important details or takes things out of context.

MOSTLY FALSE – The statement contains an element of truth but ignores critical facts that would give a different impression.

FALSE – The statement is not accurate.

PANTS ON FIRE – The statement is not accurate and makes a ridiculous claim.

The burden of proof is on the speaker, and we rate statements based on the information known at the time the statement is made.

Below is a breakdown of all the ratings in PF textualRating. There are 

2,527 Normal claims (those rated with on of the 6 standardized ratings)

354 Anomalous claims (those with a non-standardized text based rating) "
```{r orig text ratings, warning=FALSE, message=FALSE, echo = FALSE}
v8_pf %>%
  group_by(textualRating) %>%
  count() %>%
  arrange(desc(n)) %>%
  kable(caption = "Textual Ratings")
```

When we reviewed the list of anomaly claims, we noticed that all came from PolitiFact articles. PolitiFact values their Truth-O-Meter ratings and use them only when they feel they can convey some level of certainty in the rating given. Truth-O-Meter ratings require a high threshold of proof. When that is lacking or there is not evidence to do a full scale fact-check the existing facts are published in an article. Fact-checks of debates and speeches are frequently written up in articles. Since PF did not feel comfortable enough to deliver a full Truth-O-Meter rating neither did we and all claims that were not given on the 6 standard ratings in textualRating were removed from the dataset. 

The new dataset was called pf_noanomaly

```{r use pF ratings, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf <- v8_pf %>%
  filter(textualRating == "TRUE" |
           textualRating == "Mostly True" |
           textualRating == "Half True" |
           textualRating == "Mostly False" |
           textualRating == "FALSE" |
           textualRating == "Pants on Fire" )

v9_pf <- v9_pf %>%
  mutate(textualRating = case_when(textualRating == "TRUE" ~ "True",
                   textualRating == "FALSE" ~ "False",
                   textualRating == "Mostly True" ~ "Mostly True",
                   textualRating == "Half True" ~ "Half True",
                   textualRating == "Mostly False" ~ "Mostly False",
                   textualRating == "Pants on Fire" ~ "Pants on Fire"))

v9_pf$textualRating <- factor(v9_pf$textualRating, 
                              levels = c("Pants on Fire", 
                                         "False", 
                                         "Mostly False", 
                                         "Half True", 
                                         "Mostly True", 
                                         "True"))

v9_pf %>%
  group_by(textualRating) %>%
  count() %>%
  kable(caption = "PF Claims with Truth-O-Meter Ratings")
```

```{r pf people, warning=FALSE, message=FALSE, echo = FALSE}
v9_pf %>%
  group_by(claimant) %>%
  count() %>%
  arrange(desc(n))
```



